{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2155a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18293ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: ..\\backend\\data\\cbc_finetune_dataset.jsonl\n",
      "MODEL OUTPUTS: ..\\backend\\models\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 3: Paths and config -----\n",
    "BASE_DIR = Path(\"..\") / \"backend\"\n",
    "DATA_JSONL = BASE_DIR / \"data\" / \"cbc_finetune_dataset.jsonl\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Quick config: change these depending on hardware\n",
    "USE_SMALL_MODEL = False \n",
    "SMALL_MODEL_NAME = \"gpt2\"\n",
    "# Example LLaMA-2 HF model id (requires HF access and compatibility)\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "OUTPUT_NAME = \"cbc_llama_finetuned\"\n",
    "\n",
    "\n",
    "print(\"DATA:\", DATA_JSONL)\n",
    "print(\"MODEL OUTPUTS:\", MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c0b6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset rows: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Generate a CBC-aligned question for Grade 4 Mathematics based on code M4.1.1 at the 'Remember' Bloom level.\",\n",
       " 'completion': \"Question: What is 5 + 7? Options: ['10',  '11',  '12',  '13']. Answer: 12\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Cell 4: Load dataset (jsonl) -----\n",
    "assert DATA_JSONL.exists(), f\"Dataset not found at {DATA_JSONL}. Run Step 2 first.\"\n",
    "\n",
    "\n",
    "# datasets can read json lines directly\n",
    "raw_ds = load_dataset('json', data_files=str(DATA_JSONL), split='train')\n",
    "print(\"Loaded dataset rows:\", len(raw_ds))\n",
    "raw_ds = raw_ds.shuffle(seed=42)\n",
    "raw_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3623bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 12\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 5: Prepare text column (concatenate prompt+completion) -----\n",
    "# For causal LM training we'll create a single text doc with prompt + completion, optionally with special separator tokens\n",
    "SEP = \"\\n--\\n\"\n",
    "\n",
    "def to_text(example):\n",
    "    prompt = example.get('prompt', '').strip()\n",
    "    completion = example.get('completion', '').strip()\n",
    "    # We will train the model to predict the completion given the prompt. For simplicity in this notebook\n",
    "    # we concatenate prompt + SEP + completion and later use labels masking if you want to only compute loss on completion.\n",
    "    return prompt + SEP + completion\n",
    "\n",
    "text_ds = raw_ds.map(lambda x: {\"text\": to_text(x)}, remove_columns=raw_ds.column_names)\n",
    "text_ds = text_ds.train_test_split(test_size=0.05, seed=42)\n",
    "print(text_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e83af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e3443a280b4b4bbc2a39046b748a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 6: Tokenizer & Model (LLaMA-2) -----\n",
    "\n",
    "# ✅ Step 1: Ensure dependencies are installed\n",
    "# Run this once if not already installed\n",
    "# !pip install sentencepiece tiktoken huggingface_hub[hf_xet] --quiet\n",
    "\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ✅ Step 2: Fix Windows symlink warnings (optional)\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# ✅ Step 3: Define model name — choose one you have access to\n",
    "# Use 7B unless you have access to and resources for 70B\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "model_name = LLAMA_MODEL_NAME\n",
    "print(\"Using model:\", model_name)\n",
    "\n",
    "# ✅ Step 4: Load tokenizer — LLaMA tokenizers can have SentencePiece issues if 'use_fast=True'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# Some LLaMA tokenizers don't define a pad token — map pad to eos\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "# ✅ Step 5: Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Resize model embeddings if tokenizer added tokens\n",
    "if len(tokenizer) != model.get_input_embeddings().weight.shape[0]:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(\"✅ Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5908fc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29898654ed9d40189c5b06587b23b402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faeda631f2054c3ab34b481257cef579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenized dataset example:\n",
      "{'input_ids': [1, 3251, 403, 263, 315, 5371, 29899, 13671, 1139, 363, 4989, 311, 29871, 29946, 4223, 2729, 373, 775, 382, 29946, 29889, 29896, 29889, 29941, 472, 278, 525, 29177, 1689, 29915, 11447, 290, 3233, 29889, 13, 489, 13, 16492, 29901, 8449, 310, 278, 1494, 338, 263, 302, 1309, 29973, 25186, 29901, 6024, 3389, 742, 29871, 525, 29882, 14862, 742, 29871, 525, 27041, 742, 29871, 525, 24561, 368, 13359, 673, 29901, 3762], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 7: Tokenize -----\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Apply tokenization to the entire dataset\n",
    "tokenized = text_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Preview one sample\n",
    "print(\"✅ Tokenized dataset example:\")\n",
    "print(tokenized[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b29f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Cell 8: Data Collator -----\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bff0559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Cell 9: Training Arguments -----\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define output directory for your fine-tuned model\n",
    "output_dir = str(MODELS_DIR / OUTPUT_NAME)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,      # Use batch size 1 for large models\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,      # Simulate larger effective batch\n",
    "    num_train_epochs=1,                 # Try 1 epoch first\n",
    "    fp16=True,                          # Use half precision if on GPU\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",              # ← old versions use this instead of evaluation_strategy\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,                 # Keep max 2 checkpoints\n",
    "    save_steps=200,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"                    # Disable WandB/TensorBoard\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69de12af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shirl\\AppData\\Local\\Temp\\ipykernel_11508\\62486159.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 10: Trainer -----\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=tokenized['train'],\n",
    "eval_dataset=tokenized['test'],\n",
    "tokenizer=tokenizer,\n",
    "data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e61a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
      "c:\\Users\\shirl\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ----- Cell 11: Start training -----\n",
    "# NOTE: This will run locally. If you have no GPU set USE_SMALL_MODEL=True and keep epochs small.\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
